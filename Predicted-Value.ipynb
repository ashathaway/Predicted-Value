{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted Value Project for Cross Sell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anna Hathaway\n",
    "* Goals: predict if a customer is going to increase products in their insurance coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import cv\n",
    "from dython.nominal import identify_nominal_columns\n",
    "from dython.nominal import associations\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing renewal data\n",
    "renewal_df = pd.read_pickle(\"c:/Users/H008906/OneDrive - Principal Financial Group/expected value/renewal_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting columns form renewal data \n",
    "renewal_df['RENEWAL_DATE'] = pd.to_datetime(renewal_df['RENEWAL_DATE'])\n",
    "# renewal_df['COVERAGE_START_YEAR'] = renewal_df['COVERAGE_EFFECTIVE_DATE'].dt.year\n",
    "# renewal_df['RENEWAL_YEAR'] = renewal_df['RENEWAL_DATE'].dt.year\n",
    "# renewal_df['COVERAGE_DURATION'] = renewal_df['RENEWAL_YEAR'] - renewal_df['COVERAGE_START_YEAR']\n",
    "renewal_df['RTN'] = renewal_df['RENEWED_CURRENT_PREMIUM'] / renewal_df['RENEWED_NEEDED_PREMIUM']\n",
    "renewal_df['RTN'].fillna(0, inplace = True)\n",
    "\n",
    "cols_to_use = ['CASE','PRODUCT','RENEWAL_DATE','ENTERING_CURRENT_PREMIUM','DENTAL','LIFE','LTD','STD','PRODUCTSCOUNT','RG','BROKER_STATUS','RTN']\n",
    "\n",
    "cols_not_to_use = ['Unnamed: 0','SALES_OFFICE','OFFICE','ALLIANCE','BAND','BAND_GROUP','CURRENT_BAND','BAND_GROUP2','RATE_CHANGE_BAND','ENTERING_RTGT_COUNT',\n",
    "'ENTERING_CALC_WT','ENTERING_CALC_PROFIT','RENEWED_RTGT_COUNT','RENEWED_CALC_WT','RENEWED_USED_WT','RENEWED_PREM_SPEND','RENEWED_CALC_PROFIT','LAPSED_CVG_COUNT',\n",
    "'LAPSED_CURRENT_PREMIUM','LAPSED_LIVES','LAPSED_RTGT_COUNT','LAPSED_RTGT_PREMIUM','LAPSED_CALC_WT','LAPSED_NEEDED_PREMIUM','LAPSED_TARGET_PROFIT','LAPSED_CURRENT_PROFIT',\n",
    "'LAPSED_CALC_PROFIT','NO_CHANGE','RENEWED_AMEND_WT','RENEWED_MPD_WT', 'ENTERING_CVG_COUNT','CASE_SIZE_GROUP','CASE_SIZE_GROUP2','CASE_SIZE_GRP3','RENEWED_CVG_COUNT',\n",
    "'RENEWED_CURRENT_PREMIUM','RENEWED_LIVES','RENEWED_RTGT_PREMIUM','RENEWED_NEEDED_PREMIUM',\n",
    "'RENEWED_TARGET_PROFIT','RENEWED_CURRENT_PROFIT', 'RENEWED_USED_PROFIT','RENEWED_RENEWAL_PREMIUM', 'PERSISTED','DURATION_GROUP','RENEWAL_MONTH','BROKER_PAYEE', 'BROKER', \n",
    "'ENTERING_TARGET_PROFIT','ENTERING_CURRENT_PROFIT','ENTERING_USED_PROFIT','ENTERING_LIVES','RENEWAL_YEAR','COVERAGE_START_YEAR','COVERAGE_EFFECTIVE_DATE']\n",
    "\n",
    "renewal_df = renewal_df[cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group renewal data by case/renewal date then create target cross sell variable \n",
    "renewal_grouped = renewal_df.groupby(['CASE','RENEWAL_DATE'])['PRODUCT'].count().reset_index()\n",
    "renewal_grouped = renewal_grouped.rename(columns={'PRODUCT':'product_count'})\n",
    "renewal_grouped = renewal_grouped.sort_values(by=['CASE','RENEWAL_DATE'])\n",
    "renewal_grouped['count_diff'] = renewal_grouped.groupby(['CASE'])['product_count'].diff()\n",
    "renewal_grouped['cross_sell'] = renewal_grouped['count_diff'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# merge new grouped data with target variable data with renewal data\n",
    "renewal = pd.merge(renewal_df, renewal_grouped, on=['CASE','RENEWAL_DATE'], how='inner')\n",
    "\n",
    "# add new RG column\n",
    "renewal['RG_binary'] = renewal['RG'].apply(lambda x: 1 if x=='RG' else 0)\n",
    "\n",
    "# create dataset with one row per case\n",
    "renewal_agg = renewal.groupby(['CASE']).aggregate(\n",
    "    {'ENTERING_CURRENT_PREMIUM':'mean', \n",
    "     'DENTAL':'max', 'LIFE':'max', 'LTD':'max', 'STD':'max',\n",
    "     'PRODUCTSCOUNT':'max',\n",
    "     #'COVERAGE_DURATION':'mean',\n",
    "     'cross_sell':'max',\n",
    "     'RG_binary':'max',\n",
    "     'RTN':'mean',\n",
    "     }).reset_index()\n",
    "\n",
    "# get the broker status for each case \n",
    "grouped_df=renewal.groupby('CASE')['BROKER_STATUS']\n",
    "most_common_values=grouped_df.apply(lambda x: x.mode().iloc[0])\n",
    "most_common_values = most_common_values.reset_index()\n",
    "# need to make all statuses lowercase to match \n",
    "most_common_values['BROKER_STATUS'] = most_common_values['BROKER_STATUS'].apply(str.lower)\n",
    "# combine silver and elite statuses together because they are correlated\n",
    "most_common_values['BROKER_STATUS'] = np.where(most_common_values['BROKER_STATUS'] == 'silver', 'silver/elite', most_common_values['BROKER_STATUS'])\n",
    "most_common_values['BROKER_STATUS'] = np.where(most_common_values['BROKER_STATUS'] == 'elite', 'silver/elite', most_common_values['BROKER_STATUS'])\n",
    "# get the dummies for broker status and store it in a variable then change to 0 1\n",
    "broker_dummies = pd.get_dummies(most_common_values.BROKER_STATUS)\n",
    "broker_dummies = broker_dummies.astype(int)\n",
    "# Concatenate the dummies to original dataframe\n",
    "most_common_values_dum = pd.concat([most_common_values, broker_dummies], axis=1)\n",
    "# drop the other status column \n",
    "most_common_values_dum = most_common_values_dum.drop(['BROKER_STATUS', 'not available', 'missing info', 'multiple statuses', 'gold', 'n/a', 'platinum'], axis=1)\n",
    "# merge with aggregated data\n",
    "renewal_final = pd.merge(renewal_agg, most_common_values_dum, on=['CASE'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the aqua json file\n",
    "aqua_df = pd.read_pickle(\"c:/Users/H008906/OneDrive - Principal Financial Group/expected value/qdf.pkl\")\n",
    "aqua_df = aqua_df[aqua_df['stat_cd'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting columns from aqua_df\n",
    "cols_to_use = ['contract', 'tot_lvs_cnt', 'sic_cd']\n",
    "\n",
    "cols_not_use = ['grp_rprst_nmbr', 'grp_busn_id', 'vrsn_seq_nmbr', 'acq_prpsl_id', 'acq_vrsn_nmbr', 'st_cd', 'stat_cd', 'prfx_cd', 'grp_ofc_nmbr','mktr_nm', 'mktr_ofc_nm','medical',\n",
    "                'rx', 'all_products','add','gul_life','reimbursemnt_arngmnt','health_savings_acct','aggregate_xcss_loss','vtl_add','wellness_program',\n",
    "                'specific_xcss_loss', 'ltd','std vision','dental','supp_life','vtl_old','dep_life', 'life_add','vdp','supplife_add','vtl_add_old','child_vtl','spouse_vtl',\n",
    "                'vtl','vol_critical_illness','accident', 'effective_dt','prem_amt']\n",
    "\n",
    "aqua_df = aqua_df[cols_to_use]\n",
    "aqua_df['contract']=aqua_df['contract'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the renewal data and aqua file\n",
    "renewal_aqua = pd.merge(renewal_final, aqua_df, left_on='CASE', right_on='contract',how='inner')\n",
    "renewal_aqua = renewal_aqua.drop('contract', axis = 1)\n",
    "# get the current premium per life\n",
    "renewal_aqua['premium_per_life'] = renewal_aqua['ENTERING_CURRENT_PREMIUM'] / renewal_aqua['tot_lvs_cnt']\n",
    "renewal_aqua = renewal_aqua.drop(['ENTERING_CURRENT_PREMIUM', 'tot_lvs_cnt'], axis=1)\n",
    "\n",
    "# counting cases per each sic code \n",
    "renewal_aqua['sic_count']  = renewal_aqua.groupby('sic_cd')['CASE'].transform('count')\n",
    "# creating bins for each sic code group\n",
    "bin_labels = ['sic_1', 'sic_2', 'sic_3']\n",
    "renewal_aqua['sic_bin'] = pd.qcut(renewal_aqua['sic_count'], q=3, labels=bin_labels)\n",
    "# dropping unused sic code columns\n",
    "renewal_aqua = renewal_aqua.drop(['sic_cd', 'sic_count'], axis=1)\n",
    "\n",
    "# get the dummies for each sic code and store it in a variable then change to 0 1\n",
    "sic_dummies = pd.get_dummies(renewal_aqua.sic_bin)\n",
    "sic_dummies = sic_dummies.astype(int)  \n",
    "# concatenate the dummies to original dataframe\n",
    "renewal_aqua_dum = pd.concat([renewal_aqua, sic_dummies], axis=1)\n",
    "# drop values\n",
    "renewal_aqua = renewal_aqua_dum.drop(['sic_bin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def limits_function(data, column, min_obs, min_obs_sold, min_perc, max_perc):\n",
    "        '''helper function for categorical_feature_limits function\n",
    "            function to run for each categorical variable that is desired to have limits\n",
    "            min_obs is a minimum number of observations required for category to be considered\n",
    "            min_obs_sold is a minimum number of sales required for category to be considered\n",
    "            min_perc, max_perc (in decimal format) are minimum and maximum close ratios on quotes for category to be considered\n",
    "            all non-considered categories put into \"Other\" category'''\n",
    "\n",
    "        #set column data type to category\n",
    "        data[column] = data[column].astype('category')\n",
    "        #category count\n",
    "        items = data[column].value_counts().to_dict().items()\n",
    "        #category sold count\n",
    "        items_sold = data[data['cross_sell']==1][column].value_counts().to_dict().items()\n",
    "        #category close ratio \n",
    "        items_perc = (data[data['cross_sell']==1].groupby([data[data['cross_sell']==1][column]])[column].count() / \n",
    "                    data.groupby([column, 'cross_sell'])[column].count().groupby(level=0).sum()).to_dict().items()\n",
    "        #add Other category for column as possible category\n",
    "        data[column]=data[column].cat.add_categories(['Other'])\n",
    "        #if category does not meet criteria, reset to Other category\n",
    "        data.loc[(data[column].isin([key for key, val in items if val < min_obs]))|\n",
    "                (data[column].isin([key for key, val_sold in items_sold if val_sold < min_obs_sold]))|\n",
    "                (data[column].isin([key for key, val in items_perc if not min_perc < val < max_perc])), column] = 'Other'\n",
    "        data[column] = data[column].fillna('Other')\n",
    "        #for categories that were moved to Other, delete\n",
    "        data[column]=data[column].cat.remove_unused_categories()\n",
    "\n",
    "        return data[column]\n",
    "\n",
    "\n",
    "    def categorical_feature_limits(data):\n",
    "        '''for the categorical features, some categories have very sparse examples - \n",
    "            limit use of categories to only those with an acceptable amount of examples\n",
    "            uses limits_function to perform the limits'''\n",
    "\n",
    "        data['sic_cd'] = limits_function(data, 'sic_cd', 5000, 100, 0.01, 0.4)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting columns into categories \n",
    "def encode(data):\n",
    "        columns=['cross_sell', 'DENTAL', 'LIFE', 'LTD', 'STD', 'PRODUCTSCOUNT','RG_binary', 'silver/elite', 'sic_1', 'sic_2', 'sic_3']\n",
    "        for cat in columns:\n",
    "            data[cat] = data[cat].astype('category')\n",
    "\n",
    "        return data\n",
    "        \n",
    "model_data = encode(renewal_aqua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variables correlation matrix\n",
    "# dropping variables not used in modeling\n",
    "data_nominal = model_data.drop(['CASE'], axis=1)\n",
    "# identifying nomial columns \n",
    "cat =identify_nominal_columns(data_nominal)\n",
    "selected_column= data_nominal[cat]\n",
    "# plotting heat map\n",
    "categorical_correlation= associations(selected_column, filename= 'categorical_correlation.png', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominal variables correlation matrix \n",
    "data_ordinal = model_data.drop(['CASE', 'cross_sell', 'DENTAL', 'LIFE', 'LTD', 'STD', 'PRODUCTSCOUNT', 'RG_binary', 'silver/elite',  'sic_1', 'sic_2', 'sic_3'], axis=1)\n",
    "# plotting correlation heatmap\n",
    "dataplot = sns.heatmap(data_ordinal.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "# displaying heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the independent ordinal variables set\n",
    "X = data_ordinal\n",
    "  \n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                          for i in range(len(X.columns))]\n",
    "  \n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resursive feature elimination \n",
    "def rfecv(self):\n",
    "        SEED=42\n",
    "        data=self\n",
    "        X=data.drop(['cross_sell', 'CASE'], axis=1)\n",
    "        y=data['cross_sell']\n",
    "        ordinal_encoder=OrdinalEncoder()\n",
    "        for col in X.select_dtypes(include='object'):\n",
    "            X[col]=ordinal_encoder.fit_transform(X[col])\n",
    "        model=RandomForestClassifier(random_state=SEED)\n",
    "        rfecv=RFECV(estimator=model, cv=5)\n",
    "        X_rfe=rfecv.fit_transform(X,y)\n",
    "        selected_features=X.columns[rfecv.support_].to_list()\n",
    "        print('Selected Features:')\n",
    "        print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfecv(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folds and seeds\n",
    "folds = 5 \n",
    "seed=0\n",
    "kf = KFold(n_splits = folds, shuffle = True, random_state=seed)\n",
    "\n",
    "# target variable \n",
    "model_data['cross_sell'] = model_data['cross_sell'].astype('bool')\n",
    "\n",
    "# split data into train and test\n",
    "train, test = np.split(model_data.sample(frac=1, random_state=seed), [int(.8*len(model_data))])\n",
    "\n",
    "\n",
    "# define features and target \n",
    "y_train = train['cross_sell']\n",
    "X_train = train.drop(['cross_sell', 'CASE'], axis=1)\n",
    "X_train_index = train['CASE']\n",
    "y_test = test['cross_sell']\n",
    "X_test = test.drop(['cross_sell', 'CASE'], axis=1)\n",
    "X_test_index = test['CASE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the category columns \n",
    "catCols = [i for i,v in enumerate(X_train.dtypes) if str(v)=='category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm model\n",
    "for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "    train_x, train_y = X_train.iloc[train_idx],y_train.iloc[train_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    lgb_train = lgb.Dataset(train_x, label=train_y)\n",
    "    lgb_val = lgb.Dataset(val_x, label=val_y, reference=lgb_train)\n",
    "    params_lgb = {\n",
    "        'boosting_type': 'dart',\n",
    "        'metric': ['auc','f1','binary_error'],\n",
    "        'objective':'binary',\n",
    "        'bagging_freq': 0,\n",
    "        'num_boost_round': 500,\n",
    "        'verbose': 0,\n",
    "        'is_unbalance': True,\n",
    "        'path_smooth':0.5,\n",
    "        'num_terations':500,\n",
    "        'early_stopping_round':100,\n",
    "        'extra_trees':True,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'feature_fraction': 0.4,\n",
    "        'learning_rate': 0.9,\n",
    "        'max_bin': 80,\n",
    "        'max_depth': 25,\n",
    "        'min_data_in_leaf': 70,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'num_leaves': 40,\n",
    "        'subsample': 0.25\n",
    "        }\n",
    "    lgbm = lgb.train(params_lgb,lgb_train, valid_sets=lgb_val, categorical_feature=catCols)\n",
    "    feature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importance(),X_train.columns)), columns=['Value','Feature'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting lgbm\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report for lgbm model \n",
    "model_test = lgbm.predict(X_test)\n",
    "model_train = lgbm.predict(X_train)\n",
    "ypred = [int(p>=0.6) for p in model_test]\n",
    "print(classification_report(y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal parameters \n",
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, \n",
    "                            output_process=False):\n",
    "    '''function for using bayesian optimization to determine optimal training parameters for lightgbm model'''\n",
    "    \n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
    "        params = {'application':'binary', 'metric':'auc'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        \n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, metrics=['auc'])\n",
    "        #print(cv_result)       \n",
    "        return max(cv_result['valid auc-mean'])\n",
    "     \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (24, 80),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin':(20,90),\n",
    "                                            'min_data_in_leaf': (20, 80),\n",
    "                                            'min_sum_hessian_in_leaf':(0,100),\n",
    "                                           'subsample': (0.01, 1.0)}, random_state=200)\n",
    "\n",
    "    \n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. \n",
    "    #init_points: How many steps of random exploration you want to perform.\n",
    "    \n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    mae=[]\n",
    "    for model in range(len( lgbBO.res)):\n",
    "        mae.append(lgbBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(mae).idxmin()]['target'],lgbBO.res[pd.Series(mae).idxmin()]['params']\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, \n",
    "                                     init_round=5, opt_round=10, n_folds=folds, random_seed=seed ,n_estimators=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of optimal parameters \n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['boosting_type']= 'dart'\n",
    "opt_params[1]['objective']='regression'\n",
    "opt_params[1]['metric']= 'mae'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['num_class']=y_train.nunique()+1\n",
    "opt_params=opt_params[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
